# Introduction

GPT-Valkyrie is my independent AI research project where I reproduced GPT-2 from scratch using PyTorch and conducted ablation studies on normalization techniques. I decided to force myself into researching GPTs as my IB Extended Essay in January 2024, because I wanted to deeply understand how AI works.

The project involved training multiple model variants on Nvidia A100 and H100 GPUs, experimenting with different normalization layers (Layer Normalization, RMS Normalization, and Power Normalization), and evaluating models using GPT-4o as a gold-standard evaluator.

---

# Ablation Studies

<MyCarousel
  slides={[
    {
      src: "/content/portfolio/computer-science/projects/gpt-valkyrie/images/GPT-Valkyrie - Ablation Studies.png",
      caption: "Ablation study architecture variants"
    }
  ]}
  width={900}
  height={600}
/>

In the original decoder-only transformer architecture (GPT-2, which I reproduced using a 900-line PyTorch code based on Andrej Karpathy's nanoGPT repo and "Zero-to-Hero" course), I investigated whether normalisation layers are truly necessary for smaller models at 124 million to 1.5 billion parameters.

For my investigation, I created 4 variant models by systematically removing or replacing normalization layers:
- **BaseModel**: Original GPT-2 architecture with Layer Normalization
- **noNorm**: LayerNorm replaced with identity function f(x)=x
- **RMSN**: Root Mean Square Normalization (used by LLaMa)
- **PN**: Power Normalization (proposed by UC Berkeley researcher)

<br></br>
<br></br>

---

# Training Infrastructure

<MyCarousel
  slides={[
    {
      src: "/content/portfolio/computer-science/projects/gpt-valkyrie/images/nvidia-a100.jpg",
      caption: "Nvidia A100 GPU cluster"
    },
    {
      src: "/content/portfolio/computer-science/projects/gpt-valkyrie/images/training-setup.jpg",
      caption: "Training workflow setup"
    },
    {
      src: "/content/portfolio/computer-science/projects/gpt-valkyrie/images/workspace-1.jpg",
      caption: "Development workspace"
    }
  ]}
  width={900}
  height={600}
/>

I got access to 4x Nvidia A100 GPUs for free for 200+ hours thanks to purchasing Nvidia's GTC DNN course ($550 USD). After exhausting those credits, I switched to renting Nvidia H100 80GB SXM5 GPUs from Tensordock.

My workflow involved:
1. Testing PyTorch training code on Google Colab (Nvidia L4, $0.5/hour)
2. Once verified, running production training on Nvidia H100s
3. Logging all experiments with Weights & Biases

<br></br>
<br></br>

---

# Results & Findings

Training runs were logged using Weights & Biases. Around step 10k, the Power Normalization (PN) runs failed as the loss values started increasing from convergence - my first time seeing gradient explosion in action.

**Key findings:**
- Layer Normalization (LN) and RMS Normalization (RMSN) worked reliably as baselines
- Power Normalization (PN) was ~20% slower and used more RAM due to complex batch-norm-like operations
- PN failed mid-training due to gradient explosion, likely from division by zero in the architecture
- Interestingly, for GPT-2 small (124M parameters), models without normalization (noNorm) performed comparably to normalized variants

The PN failure was a valuable lesson in understanding why ML research is difficult to reproduce - even with full source code. The FineWeb-1.5B dataset I used was more diverse than the original researcher's Wikipedia dataset, requiring more significant weight changes that destabilized training.

<br></br>
<br></br>

---

# Evaluation with GPT-4o

To evaluate the ablated models fine-tuned on QA, Summarization and Text Generation tasks, I used GPT-4o as a gold-standard evaluator instead of traditional metrics like BLEU and ROUGE.

This approach was inspired by the limitations of traditional metrics emphasized by Andrej Karpathy and the O'Reilly NLP book. Traditional metrics logged significant differences between models, but GPT-4o identified fewer meaningful distinctions. After inspecting the raw CSV files, I agreed with GPT-4o's analysis, as its intelligent evaluation aligned more closely with real model performance.

This experiment cost $11 USD in OpenAI API credits but greatly enhanced the accuracy of my evaluation pipeline.

<br></br>
<br></br>

---

# Links & Resources

<DocumentLink
  href="https://github.com/Ice-Citron/GPT-Valkyrie/blob/main/Extended%20Essay%20-%20Transformers.pdf"
  title="Extended Essay - Transformers"
  description="Full independent research paper (IB Extended Essay format)"
  icon="ðŸ“„"
/>

<DocumentLink
  href="https://github.com/Ice-Citron/GPT-Valkyrie"
  title="GPT-Valkyrie Repository"
  description="Main GitHub repository with all training code"
  icon="ðŸ’»"
/>

<DocumentLink
  href="https://huggingface.co/shng2025"
  title="Trained Model Weights"
  description="HuggingFace repositories with trained model weights"
  icon="ðŸ¤—"
/>

**Related Learning Repositories:**
- [O'Reilly NLP Book 1](https://github.com/Ice-Citron/GPTesla)
- [O'Reilly NLP Book 2](https://github.com/Ice-Citron/NLP-Transformer)
- [Andrej Karpathy Course 1](https://github.com/Ice-Citron/GPT-dev__Andrej-course)
- [nanoGPT-Valkyrie](https://github.com/Ice-Citron/nanoGPT-Valkyrie)

---

# Skills Developed

- Deep understanding of transformer architectures (GPT-2, attention mechanisms)
- PyTorch model implementation from scratch
- GPU cluster management (A100, H100, Google Colab)
- ML experiment tracking with Weights & Biases
- Ablation study design and execution
- Using LLMs as evaluators (GPT-4o API)
- Understanding research reproducibility challenges

